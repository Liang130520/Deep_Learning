{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNDrsRXoBcY7MXSxvwKW4Zs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Liang130520/Deep_Learning/blob/main/DeepLearning_1107_3_Loss(%E6%90%8D%E5%A4%B1%E5%80%BC).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c3JpEsCOjOOD"
      },
      "outputs": [],
      "source": [
        "\" Machine Learning \" # 處理表格類資料\n",
        "\n",
        "\" 深度學習(Deep Learning) \" # 處理表格以外的資料, 屬 \"模仿\" 人類神經網路的形式\n",
        "# -> 接收端 到 輸出端, 所有係數相加, 再進行轉換輸出, 遵守\"全有全無率\"\n",
        "# --> X1 * W1 + X2 * W2 + ...... ---> WiXi \"激活(Activation)\"\" ----> 輸出, 若無激活, 則無輸出\n",
        "# 意即為 分類器 / 感知器 (Perceptron), 感知有或無的情形\n",
        "\n",
        "\" Perceptron的調整 \"\n",
        "# 為了模擬感受大小的差異, 將一條神經(程式)改成有中間值的概念\n",
        "\n",
        "\" Sigmoid \" # -> Sigmoid => y = 1 / (1 + e ** -x)\n",
        "# 為何要有 (1 +)?  ->  if y = 1 / e ** -x --> y = e ** x\n",
        "# 令 x = 0, 則 y = 1\n",
        "# 令 x = -∞, 則 y ≒ 0\n",
        "# 令 x = ∞, 則 y ≒ ∞\n",
        "# --> y = 1 / (1 + e ** -x), if x = 0, 則 y = 0.5, if x = -∞, 則 y ≒ 0, if x = ∞, 則 y ≒ 1.\n",
        "# ---> (1 +) 為設定天花板用的, if 天花板想設定為 2, 則改為 (0.5 +)即可, 以此類推\n",
        "\n",
        "\" Logistic Regression\" \n",
        "# 仍為分類器, 非回歸, Logistic 等於 Sigmoid, 為 Sigmoid 的函式名稱\n",
        "\n",
        "\" Multi-Layer Perceptron \"\n",
        "# 多層 Logistic Regression 組合, 再做額外的 Logistic Regression 轉換\n",
        "# 其中只要有用到 Logistic Regression 就必定要 \"轉換\" 輸出, 才能變成 多層結構\n",
        "# step1 : 係數隨機 (a1, a2, b1, b2, c1, c2, w1, w2, ......)\n",
        "# step2 : 調整係數, 須越靠近 \"正確\"\", 不斷調整\n",
        "# 係數該如何調整, if 要取得最低點, 則往下延伸, 如遇到上升, 則返回停下\n",
        "# -> 即取得 m(斜率), 大小代表陡峭程度, 正負代表上升方向(正往右, 負往左)\n",
        "\n",
        "\" 斜率下坡理論 \"\n",
        "# 欲往 值低方向 走, 則方向應往 負斜率 走, 因地平面斜率趨近於 0, 斜率越大, 越往反向走, 大小則應考量abs(斜率), 思考該走大步或小步.\n",
        "# 愈往 值高方向 走, 則方向應往 正斜率 走, 因地平面斜率趨近於 0, 斜率越大, 越往該方向走, 大小則應考量abs(斜率), 思考該走大步或小步.\n",
        "\n",
        "\" 多維計算方式 \" # z = 2 * x ** 2 +  3 * y ** 3, p1 = (1, 2), 要往谷底走\n",
        "\" 偏微分 \"\n",
        "# step1 : 固定某邊, 分別微分兩個平面, mx = 4 * x, my = 9 * y **2\n",
        "# step2 : (mx, my) = (4x, 9 * y ** 2), 代入(1, 2), 得(4, 36), 依斜率下坡理論, 得 -(4, 36), 取得方向(往左)及大小(4, 36)\n",
        "# 偏微分: mx = bz / bx, my = bz / by, 得(mx, my)\n",
        "\n",
        "\" Loss(損失值) \"\n",
        "# Multi-Layer Perceoptron 如何評估, ((pre - true) ** 2 + ......) / n -> Loss(損失值), 等於 MSE 計算方式\n",
        "# 將 Loss 定為 三維中的 z 維, 即可取得每個係數的調整項, a1 = -(bloss) / ba1\n",
        "# --> a1下 = a1現 - [(bloss / ba1現) * 步長]\n",
        "# 梯度下降(多口斜率) -> 定隨機維度下降"
      ]
    }
  ]
}